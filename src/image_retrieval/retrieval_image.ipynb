{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class TrainRetrievalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, data_name):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param data_name: base name of processed datasets\n",
    "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
    "        :param transform: image transform pipeline\n",
    "        \"\"\"\n",
    "        self.data_folder=data_folder\n",
    "\n",
    "        with open(os.path.join(data_folder, \"TRAIN\" + '_IMGPATHS_' + data_name + '.json'), 'r') as j:\n",
    "            self.imgpaths = json.load(j)\n",
    "\n",
    "        #self.imgpaths=self.imgpaths[:10]\n",
    "        #print(\"self images\", self.imgpaths)\n",
    "        ##TODO:REMOVE\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.imgpaths)\n",
    "        #print(\"this is the actual len on begin init\", self.dataset_size)\n",
    "\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],  # mean=IMAGENET_IMAGES_MEAN, std=IMAGENET_IMAGES_STD\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
    "        img = Image.open(self.data_folder+\"/\"+self.imgpaths[i])\n",
    "        img = self.transform(img)\n",
    "        #print(\"i of retrieval dataset\",i)\n",
    "        return img, i\n",
    "\n",
    "    def __len__(self):\n",
    "        #print(\"this is the actual len on __len\", self.dataset_size)\n",
    "        return self.dataset_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ImageRetrieval():\n",
    "\n",
    "    def __init__(self, dim_examples, encoder, train_dataloader_images, device):\n",
    "        #print(\"self dim exam\", dim_examples)\n",
    "        self.datastore = faiss.IndexFlatL2(dim_examples) #datastore\n",
    "        self.encoder= encoder\n",
    "\n",
    "        #data\n",
    "        self.device=device\n",
    "        self.imgs_indexes_of_dataloader = torch.tensor([]).long().to(device)\n",
    "        #print(\"self.imgs_indexes_of_dataloader type\", self.imgs_indexes_of_dataloader)\n",
    "\n",
    "        #print(\"len img dataloader\", self.imgs_indexes_of_dataloader.size())\n",
    "        self._add_examples(train_dataloader_images)\n",
    "        #print(\"len img dataloader final\", self.imgs_indexes_of_dataloader.size())\n",
    "        #print(\"como ficou img dataloader final\", self.imgs_indexes_of_dataloader)\n",
    "\n",
    "\n",
    "    def _add_examples(self, train_dataloader_images):\n",
    "        print(\"\\nadding input examples to datastore (retrieval)\")\n",
    "        for i, (imgs, imgs_indexes) in enumerate(train_dataloader_images):\n",
    "            #add to the datastore\n",
    "            imgs=imgs.to(self.device)\n",
    "            imgs_indexes = imgs_indexes.long().to(self.device)\n",
    "            #print(\"img index type\", imgs_indexes)\n",
    "            encoder_output = self.encoder(imgs)\n",
    "\n",
    "            encoder_output = encoder_output.view(encoder_output.size()[0], -1, encoder_output.size()[-1])\n",
    "            input_img = encoder_output.mean(dim=1)\n",
    "\n",
    "            self.datastore.add(input_img.cpu().numpy())\n",
    "\n",
    "            if i%5==0:\n",
    "                print(\"i and img index of ImageRetrival\",i, imgs_indexes)\n",
    "                print(\"n of examples\", self.datastore.ntotal)\n",
    "            self.imgs_indexes_of_dataloader= torch.cat((self.imgs_indexes_of_dataloader,imgs_indexes))\n",
    "\n",
    "\n",
    "\n",
    "    def retrieve_nearest_for_train_query(self, query_img, k=2):\n",
    "        #print(\"self query img\", query_img)\n",
    "        D, I = self.datastore.search(query_img, k)     # actual search\n",
    "        #print(\"all nearest\", I)\n",
    "        #print(\"I firt\", I[:,0])\n",
    "        #print(\"if you choose the first\", self.imgs_indexes_of_dataloader[I[:,0]])\n",
    "        nearest_input = self.imgs_indexes_of_dataloader[I[:,1]]\n",
    "        #print(\"the nearest input is actual the second for training\", nearest_input)\n",
    "        #nearest_input = I[0,1]\n",
    "        #print(\"actual nearest_input\", nearest_input)\n",
    "        return nearest_input\n",
    "\n",
    "    def retrieve_nearest_for_val_or_test_query(self, query_img, k=1):\n",
    "        D, I = self.datastore.search(query_img, k)     # actual search\n",
    "        nearest_input = self.imgs_indexes_of_dataloader[I[:,0]]\n",
    "        #print(\"all nearest\", I)\n",
    "        #print(\"the nearest input\", nearest_input)\n",
    "        return nearest_input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import fasttext\n",
    "import numpy as np\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "\n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        # Resize image to fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/starksultana/Documentos/MEIC/5o_ano/Tese/code/remote-sensing-image-captioning/experiments/encoder/inputs/TRAIN_IMGPATHS_flickr8k.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-6f87b39e8f46>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;31m# TrainRetrievalDataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m train_retrieval_loader = torch.utils.data.DataLoader(\n\u001B[0;32m---> 54\u001B[0;31m     \u001B[0mTrainRetrievalDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_folder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     55\u001B[0m     batch_size=batch_size, shuffle=True, num_workers=workers)#, pin_memory=True)\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-57b255f9bb23>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data_folder, data_name)\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_folder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_folder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_folder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"TRAIN\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'_IMGPATHS_'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdata_name\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'.json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimgpaths\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/starksultana/Documentos/MEIC/5o_ano/Tese/code/remote-sensing-image-captioning/experiments/encoder/inputs/TRAIN_IMGPATHS_flickr8k.json'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "from src.configs.datasets import FeaturesDataset\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Data parameters\n",
    "# folder with data files saved by create_input_files.py\n",
    "data_folder = '/home/starksultana/Documentos/MEIC/5o_ano/Tese/code/remote-sensing-image-captioning/experiments/encoder/inputs'\n",
    "# base name shared by data files\n",
    "data_name = 'flickr8k'\n",
    "\n",
    "# Model parameters\n",
    "emb_dim = 300  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "# sets device for model and PyTorch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", device)\n",
    "# set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Training parameters\n",
    "start_epoch = 0\n",
    "# number of epochs to train for (if early stopping is not triggered)\n",
    "epochs = 100\n",
    "# keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "epochs_since_improvement = 0\n",
    "batch_size = 32\n",
    "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "grad_clip = 5.  # clip gradients at an absolute value of\n",
    "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "best_bleu4 = 0.  # BLEU-4 score right now\n",
    "print_freq = 100  # print training/validation stats every __ batches\n",
    "fine_tune_encoder = False  # fine-tune encoder?\n",
    "checkpoint = None  # path to checkpoint, None if none\n",
    "\n",
    "# TrainRetrievalDataset\n",
    "train_retrieval_loader = torch.utils.data.DataLoader(\n",
    "    TrainRetrievalDataset(data_folder, data_name),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=workers)#, pin_memory=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        FeaturesDataset(data_folder, data_name, 'TRAIN'),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=workers)#, pin_memory=True)\n",
    "\n",
    "encoder = Encoder()\n",
    "encoder.fine_tune(fine_tune_encoder)\n",
    "encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                     lr=encoder_lr) if fine_tune_encoder else None\n",
    "\n",
    "image_retrieval = ImageRetrieval(2048, encoder, train_retrieval_loader, device)\n",
    "\n",
    "neighbors = []\n",
    "with torch.no_grad():\n",
    "    for i,img in enumerate(train_loader):\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "\n",
    "        # Forward prop.\n",
    "        imgs = encoder(imgs)\n",
    "        imgs = imgs.view(imgs.size()[0], -1, imgs.size()[-1])\n",
    "        #print(\"this was the imgs out\", imgs.size())\n",
    "        input_imgs = imgs.mean(dim=1)\n",
    "        nearest_imgs = image_retrieval.retrieve_nearest_for_train_query(input_imgs.cpu().numpy())\n",
    "        neighbors.append(nearest_imgs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}